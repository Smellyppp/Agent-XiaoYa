# main.py
# å¤šåŠŸèƒ½ç§˜ä¹¦åŠ©æ‰‹ç³»ç»Ÿï¼ˆæ³•å¾‹+å¤©æ°”ï¼‰
# åŠŸèƒ½ï¼šåŸºäºQwenæ¨¡å‹çš„ç»¼åˆåŠ©æ‰‹ç³»ç»Ÿ

import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from my_knowledge_base.vector_db import search_vector_db
from prompt_templates import construct_prompt_template
from api_integration.weather_api import get_weather_forecast
from location_extractor import LocationExtractor  # æ–°å¢å¯¼å…¥

class SecretaryAssistant:
    def __init__(self, 
                 model_path="./Qwen3-0.6B", 
                 embedding_model_path="./embedding_model/all-MiniLM-L6-v2",
                 vector_db_path="./my_knowledge_base/vector_db/faiss_index",
                 context_chunks=3,
                 max_new_tokens=1024):
        """åˆå§‹åŒ–ç§˜ä¹¦åŠ©æ‰‹ç³»ç»Ÿ"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="auto"
        )
        self.embedding_model_path = embedding_model_path
        self.vector_db_path = vector_db_path
        self.context_chunks = context_chunks
        self.max_new_tokens = max_new_tokens
        self.location_extractor = LocationExtractor()  # åˆå§‹åŒ–åœ°ç‚¹æå–å™¨
    
    def is_weather_query(self, query):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤©æ°”æŸ¥è¯¢"""
        weather_keywords = ['å¤©æ°”', 'æ°”æ¸©', 'é¢„æŠ¥', 'ä¸‹é›¨', 'ä¸‹é›ª', 'æ¸©åº¦', 'æ¹¿åº¦', 'æ°”è±¡']
        return any(keyword in query for keyword in weather_keywords)
    
    def handle_weather_query(self, query):
        """å¤„ç†å¤©æ°”æŸ¥è¯¢"""
        location = self.location_extractor.extract_location(query)
        print(f"ğŸ“ è¯†åˆ«åœ°ç‚¹: {location}")
        
        weather_info = get_weather_forecast(location)
        if weather_info["status"] == "error":
            return weather_info["formatted"], {}
        
        print(f"\nã€{location}çš„å¤©æ°”æ•°æ®ã€‘")
        print(weather_info["formatted"])
        
        prompt = construct_prompt_template(weather_info["formatted"], query)
        response = self._generate_response(prompt)
        
        return response, {"service": "weather", "location": location}

    def handle_law_query(self, query):
        """å¤„ç†æ³•å¾‹å’¨è¯¢"""
        context, results = self.retrieve_context(query)
        
        print("\nã€æ£€ç´¢åˆ°çš„æ³•å¾‹æ¡æ–‡ã€‘")
        for i, result in enumerate(results):
            print(f"[æ¡æ–‡ {i+1}]: {result['content']}\n")
        
        prompt = construct_prompt_template(context, query)
        response = self._generate_response(prompt)
        
        return response, {"service": "law"}
    
    def _generate_response(self, prompt):
        """é€šç”¨å›ç­”ç”Ÿæˆæ–¹æ³•"""
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        generated_ids = self.model.generate(
            **model_inputs,
            max_new_tokens=self.max_new_tokens
        )
        return self.tokenizer.decode(
            generated_ids[0][len(model_inputs.input_ids[0]):], 
            skip_special_tokens=True
        )

    def retrieve_context(self, query):
        """ä»å‘é‡æ•°æ®åº“æ£€ç´¢æ³•å¾‹ä¸Šä¸‹æ–‡"""
        results = search_vector_db(
            query=query,
            vector_db_path=self.vector_db_path,
            embedding_model_path=self.embedding_model_path,
            k=self.context_chunks
        )
        
        context = "æ£€ç´¢åˆ°çš„ç›¸å…³æ³•å¾‹æ¡æ–‡:\n\n"
        for i, result in enumerate(results):
            context += f"[æ¡æ–‡ {i+1}]: {result['content']}\n\n"
        
        return context, results

if __name__ == "__main__":
    assistant = SecretaryAssistant()
    print("å¤šåŠŸèƒ½ç§˜ä¹¦åŠ©æ‰‹å·²å¯åŠ¨ï¼ˆæ³•å¾‹å’¨è¯¢+å¤©æ°”æŸ¥è¯¢ï¼Œè¾“å…¥'exit'é€€å‡ºï¼‰")
    print("æ¸©é¦¨æç¤ºï¼šå¤©æ°”æŸ¥è¯¢é»˜è®¤åœ°ç‚¹ä¸ºä¸œèï¼Œå¯åœ¨æé—®ä¸­æŒ‡å®šå…¶ä»–åŸå¸‚")
    
    while True:
        user_input = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
        if user_input.lower() in ['exit', 'quit']:
            break
        
        if not user_input:
            print("âš  è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
            continue
        
        try:
            start_time = time.time()
            
            if assistant.is_weather_query(user_input):
                response, info = assistant.handle_weather_query(user_input)
                service_type = f"{info.get('location', '')}å¤©æ°”æŸ¥è¯¢"
            else:
                response, _ = assistant.handle_law_query(user_input)
                service_type = "æ³•å¾‹å’¨è¯¢"
            
            print(f"\nã€{service_type}ç»“æœã€‘")
            print(response)
            print(f"\n[ç³»ç»Ÿç»Ÿè®¡] å¤„ç†æ—¶é—´: {time.time()-start_time:.2f}s")
            
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")

    print("åŠ©æ‰‹æœåŠ¡ç»“æŸ")