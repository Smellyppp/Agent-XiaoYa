# main.py
# æ³•å¾‹é¡¾é—®åŠ©æ‰‹ç³»ç»Ÿ
# åŠŸèƒ½ï¼šåŸºäºQwenæ¨¡å‹çš„æ³•å¾‹åŠ©æ‰‹ç³»ç»Ÿ

import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from my_knowledge_base.vector_db import search_vector_db
from prompt_templates import construct_prompt_template
import api_integration.case_search as case_search  # å¯¼å…¥æœç´¢å¼•æ“æ¨¡å—

class LegalAdvisor:
    def __init__(self, 
                 model_path="./model/Qwen3-0.6B", 
                 embedding_model_path="./embedding_model/all-MiniLM-L6-v2",
                 vector_db_path="./my_knowledge_base/vector_db/faiss_index",
                 context_chunks=3,
                 max_new_tokens=1024):
        """åˆå§‹åŒ–æ³•å¾‹é¡¾é—®ç³»ç»Ÿ"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="auto"
        )
        self.embedding_model_path = embedding_model_path
        self.vector_db_path = vector_db_path
        self.context_chunks = context_chunks
        self.max_new_tokens = max_new_tokens
    
    def is_search_query(self, query):
        """åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦ç½‘ç»œæœç´¢çš„é—®é¢˜"""
        # éœ€è¦ç½‘ç»œæœç´¢çš„æƒ…å†µï¼šæœ€æ–°æ¡ˆä¾‹ã€æ”¿ç­–å˜åŒ–ã€å…·ä½“äº‹ä»¶ç­‰
        search_keywords = ['æœ€æ–°', 'æœ€è¿‘', 'æ–°é—»', 'æ¡ˆä¾‹', 'äº‹ä»¶', 'æ”¿ç­–', 'å˜åŒ–', 'æ›´æ–°', 'å‘ç”Ÿ', 'å…·ä½“','è¿‘æœŸ']
        return any(keyword in query for keyword in search_keywords)
    
    def handle_search_query(self, query):
        """å¤„ç†éœ€è¦ç½‘ç»œæœç´¢çš„é—®é¢˜"""
        print(f"ğŸ” æ­£åœ¨æœç´¢æœ€æ–°ä¿¡æ¯...")
        
        # è°ƒç”¨æœç´¢å¼•æ“è·å–æœ€æ–°ä¿¡æ¯
        search_result = case_search.search_and_extract(query)
        
        # æ‰“å°æœç´¢ç»“æœ
        print("\nã€æœç´¢å¼•æ“è¿”å›ç»“æœã€‘")
        print(search_result)
        
        # ä½¿ç”¨æœç´¢ç»“æœæ„å»ºæç¤º
        prompt = construct_prompt_template(search_result, query)
        response = self._generate_response(prompt)
        
        return response, {"service": "search"}

    def handle_law_query(self, query):
        """å¤„ç†æ³•å¾‹å’¨è¯¢"""
        context, results = self.retrieve_context(query)
        
        print("\nã€æ£€ç´¢åˆ°çš„æ³•å¾‹æ¡æ–‡ã€‘")
        for i, result in enumerate(results):
            print(f"[æ¡æ–‡ {i+1}]: {result['content']}\n")
        
        prompt = construct_prompt_template(context, query)
        response = self._generate_response(prompt)
        
        return response, {"service": "law"}
    
    def _generate_response(self, prompt):
        """é€šç”¨å›ç­”ç”Ÿæˆæ–¹æ³•"""
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        generated_ids = self.model.generate(
            **model_inputs,
            max_new_tokens=self.max_new_tokens
        )
        return self.tokenizer.decode(
            generated_ids[0][len(model_inputs.input_ids[0]):], 
            skip_special_tokens=True
        )

    def retrieve_context(self, query):
        """ä»å‘é‡æ•°æ®åº“æ£€ç´¢æ³•å¾‹ä¸Šä¸‹æ–‡"""
        results = search_vector_db(
            query=query,
            vector_db_path=self.vector_db_path,
            embedding_model_path=self.embedding_model_path,
            k=self.context_chunks
        )
        
        context = "æ£€ç´¢åˆ°çš„ç›¸å…³æ³•å¾‹æ¡æ–‡:\n\n"
        for i, result in enumerate(results):
            context += f"[æ¡æ–‡ {i+1}]: {result['content']}\n\n"
        
        return context, results

if __name__ == "__main__":
    advisor = LegalAdvisor()
    print("æ³•å¾‹é¡¾é—®åŠ©æ‰‹å·²å¯åŠ¨ï¼ˆè¾“å…¥'exit'é€€å‡ºï¼‰")
    print("æ¸©é¦¨æç¤ºï¼šæˆ‘å¯ä»¥å›ç­”æ³•å¾‹é—®é¢˜ï¼Œä¹Ÿå¯ä»¥æœç´¢æœ€æ–°æ¡ˆä¾‹å’Œäº‹ä»¶")
    
    while True:
        user_input = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
        if user_input.lower() in ['exit', 'quit']:
            break
        
        if not user_input:
            print("âš  è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
            continue
        
        try:
            start_time = time.time()
            
            if advisor.is_search_query(user_input):
                response, _ = advisor.handle_search_query(user_input)
                service_type = "æ¡ˆä¾‹æœç´¢"
            else:
                response, _ = advisor.handle_law_query(user_input)
                service_type = "æ³•å¾‹å’¨è¯¢"
            
            print(f"\nã€{service_type}ç»“æœã€‘")
            print(response)
            print(f"\n[ç³»ç»Ÿç»Ÿè®¡] å¤„ç†æ—¶é—´: {time.time()-start_time:.2f}s")
            
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")

    print("åŠ©æ‰‹æœåŠ¡ç»“æŸ")